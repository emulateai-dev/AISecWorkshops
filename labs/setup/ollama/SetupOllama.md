### **Step-by-Step Guide for Setting Up and Using Ollama**

---

### **1. Install Ollama on Linux**
1. **Visit the Official Download Page:**
   - Go to the [Ollama Linux download page](https://ollama.com/download/linux).
2. **Install Ollama Using the Provided Script:**
   - Run the installation command:
     ```bash
     curl -fsSL https://ollama.com/install.sh | sh
     ```
3. **Installation Process:**
   - The script will:
     - Install Ollama in `/usr/local`.
     - Configure necessary permissions.
     - Set up the Ollama systemd service.
     - Enable and start the service.

   **Example Output:**
   ```
   >>> Installing ollama to /usr/local
   >>> Adding ollama user to render group...
   >>> Adding ollama user to video group...
   >>> Adding current user to ollama group...
   >>> Creating ollama systemd service...
   >>> Enabling and starting ollama service...
   >>> The Ollama API is now available at 127.0.0.1:11434.
   >>> Install complete. Run "ollama" from the command line.
   ```

4. **Note for CPU-Only Systems:**
   - Ollama can run in CPU-only mode if you donâ€™t have a GPU, but performance may be slower.

---

### **2. Select and Run a Model**
1. **Browse Models:**
   - Visit the [Ollama Search page](https://ollama.com/search) to explore available models.
2. **Run a Model Locally:**
   - Choose a model, and run it with:
     ```bash
     ollama run <model_name>
     ```
     Example:
     ```bash
     ollama run qwen2:0.5b
     ```
   **Example Output:**
   ```
   pulling manifest 
   pulling 8de95da68dc4... 100%
   verifying sha256 digest 
   writing manifest 
   success 
   >>> hello
   Hello! How can I assist you today?
   ```
3. **Exit a Session:**
   - Type `/bye` to end the interaction.

---

### **3. Access Models Using APIs**
1. **Generate Text:**
   - Use `curl` to interact with models via API:
     ```bash
     curl http://localhost:11434/api/generate -d '{
       "model": "qwen2:0.5b",
       "prompt": "Who are you?",
       "stream": false
     }'
     ```
   **Example JSON Response:**
   ```json
   {
     "model": "qwen2:0.5b",
     "response": "As an AI, I am designed to operate and interact with users...",
     "done": true,
     "done_reason": "stop"
   }
   ```

2. **Run a Larger Model:**
   ```bash
   ollama run llama3.2:1b
   ```
   **Example Output:**
   ```
   pulling manifest 
   success 
   >>> hello
   Hello. Is there something I can help you with or would you like to chat?
   >>> Summarize following
   There is no text to summarize. This is the beginning of our conversation.
   >>> /bye
   ```

---

### **4. Customize a Model**
1. **Create a Modelfile:**
   - Customize parameters like temperature and system messages:
     ```plaintext
     FROM qwen2:0.5b
     PARAMETER temperature 1
     SYSTEM """
     You are the assistant that always follows responsible AI policies and politely denies any toxic, harmful prompts.
     """
     ```
2. **Build the Customized Model:**
   - Run the following command to create the customized model:
     ```bash
     ollama create <custom_model_name> -f Modelfile
     ```
   Example:
     ```bash
     ollama create qwen1-rai:0.5b -f Modelfile
     ```
3. **Run the Customized Model:**
   - Test the new model:
     ```bash
     ollama run <custom_model_name>
     ```

---

### **5. Manage Ollama Service**
1. **Check the Service Status:**
   ```bash
   systemctl status ollama
   ```
2. **Restart the Service:**
   ```bash
   sudo systemctl restart ollama
   ```
3. **Stop the Service When Not in Use:**
   ```bash
   systemctl stop ollama
   ```

---

### **6. Version Control for Models**
1. **Organize Modelfiles:**
   - Save `Modelfile` in a directory for version control:
     ```bash
     mkdir ollama
     mv Modelfile ollama/Modelfile-<custom_model_name>
     ```
2. **Commit Changes:**
   - Use Git to track and manage Modelfile versions:
     ```bash
     git add ollama/
     git commit -m "Added customized Modelfile for <custom_model_name>"
     ```


### **7. Run ollama to listen on 0.0.0.0"

Here are the quickest one-liner commands to stop, reconfigure, and restart Ollama to listen on `0.0.0.0`, depending on how you prefer to run it.

### Option 1: The Permanent Systemd One-Liner (Recommended)

Since you are on a Linux host, Ollama is likely running as a background systemd service. Run this single chained command to automatically create the override file, apply the `0.0.0.0` environment variable, reload the daemon, and restart the service:

```bash
sudo mkdir -p /etc/systemd/system/ollama.service.d && echo -e "[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"" | sudo tee /etc/systemd/system/ollama.service.d/override.conf > /dev/null && sudo systemctl daemon-reload && sudo systemctl restart ollama

```

*(Once this completes, Ollama will permanently listen on all interfaces, even after server reboots).*

### Option 2: The Manual CLI Approach

If you prefer to run Ollama directly in your terminal so you can watch the live logs, use this command to stop the background systemd service and instantly spin it up manually on all interfaces:

```bash
sudo systemctl stop ollama && OLLAMA_HOST=0.0.0.0 ollama serve

```